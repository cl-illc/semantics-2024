-
  layout: lecture
  selected: y
  date: 2024-04-02
  img: introduction-icon_1-267x300
  uid: intro
  title: "Introduction: Learning word and sentence representations"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this introductory lecture I will give an overview of the course and we will discuss learning word and sentence representations from text."
  background:
  discussion:
  slides: resources/slides/Sem2024-lecture1.pdf
  further: 
   -  "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. [A large annotated corpus for learning natural language inference](https://arxiv.org/pdf/1508.05326.pdf). arXiv preprint arXiv:1508.05326, 2015."
   - "Alexis Conneau and Douwe Kiela. [Senteval: An evaluation toolkit for universal sentence representations](https://www.aclweb.org/anthology/L18-1269.pdf). In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), 2018."
   - "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. [Supervised learning of universal sentence representations from natural language inference data](https://arxiv.org/pdf/1705.02364.pdf). arXiv preprint arXiv:1705.02364, 2017."
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2024-04-05
  img: bert
  uid: l2
  title: "Attention and transformers"
  instructor: "Phillip Lippe and Ekaterina Shutova"
  note: 
  abstract: "In this session we will introduce attention and transformer architectures"
  background:
  discussion:
  slides: resources/slides/ATCS_2024_Attention_Mechanisms.pdf
  further:
    - "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. [Attention Is All You Need](https://arxiv.org/abs/1706.03762). In Proceedings of NIPS 2017."
    - "A very helpful [blog post](https://jalammar.github.io/illustrated-transformer/) explaining the transformer architecture."
    - "Visualization of attention heads for BERT: [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)"
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2024-04-09
  img: BERT
  uid: l3
  title: "Seminar: The BERT model"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss the BERT model."
  background:
  discussion: 
    - "In this session we will discuss the following papers:" 
    - "Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. 2019. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf). In Proceedings of NAACL 2019."
    - "Ian Tenney, Dipanjan Das, Ellie Pavlick. 2019.  [BERT Rediscovers the Classical NLP Pipeline](https://www.aclweb.org/anthology/P19-1452/). In Proceedings of ACL 2019."
  slides:
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2024-04-12
  img: GNN
  uid: l4
  title: "Seminar: Model pruning and modularity"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss recent techniques for structured and unstructured pruning and finding task-specific subnetworks in Transformer models."
  background:
  discussion: 
    - "In this session we will discuss the following papers:"
    - "Paul Michel, Omer Levy, Graham Neubig. [Are Sixteen Heads Really Better than One?](https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf) In Proceedings of NeuroIPS 2019."
    - "Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin. [The Lottery Ticket Hypothesis for Pre-trained BERT Networks.](https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf) In Proceedings of NeuroIPS 2020."
  slides: 
  further:
    - "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. (2020). [Gradient surgery for multi-task learning](https://arxiv.org/pdf/2001.06782.pdf). arXivpreprint arXiv:2001.06782."  
-
  layout: lecture
  selected: y
  date: 2024-04-16
  img: Multilingual
  uid: l5
  title: "Multilingual models"
  instructor: "Rochelle Choenni and Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss learning multilingual word and sentence representations."
  background:
  further:
    - "The paper that introduced byte pair encoding: Rico Sennrich, Barry Haddow, Alexandra Birch. 2016. [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf). In Proceedings of ACL 2016."
    - "Zirui Wang, Zachary C. Lipton, Yulia Tsvetkov (2020). [On Negative Interference in Multilingual Language Models](https://www.aclweb.org/anthology/2020.emnlp-main.359.pdf). In the proceedings of EMNLP 2020."
    - "Mikel Artetxe and Holger Schwenk. 2018. [Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond.](https://arxiv.org/pdf/1812.10464.pdf) Transactions of the Association for Computational Linguistics."
  discussion: 
  slides: 
  further:
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2024-04-19
  img: T0
  uid: l6
  title: "Seminar: LLMs: Instruction-tuning and prompting"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss recent research on generative LMs, prompting, instruction-tuning and in-context learning."
  background:
  discussion: 
    - "In this session we will discuss the following papers:"
    - "Sanh et al., 2022. [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207.pdf). In Proceedings of ICLR 2022."
  slides: 
  further: 
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2024-04-23
  img: ICL
  uid: l7
  title: "Seminar: In-context learning"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss recent research on generative LMs, prompting, instruction-tuning and in-context learning."
  background:
  discussion: 
    - "In this session we will discuss the following papers:"
    - "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer. 2022. [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/pdf/2202.12837.pdf) In Proceedings of EMNLP 2022."
  slides: 
  further: 
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2024-04-26
  img: CoT
  uid: l8
  title: "Making LLMs Safe: A case study of LLaMA-2"
  instructor: "Pushkar Mishra, Meta AI"
  note: 
  abstract: "In this guest lecture we will discuss safety and alignment of LLMs."
  background:
  discussion: 
  slides: 
  further: 
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2023-05-07
  img: MechInt
  uid: l9
  title: "Seminar: (Mechanistic) Interpretability in NLP"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss reseach on interpretability of LLM computations."
  background:
  further: 
  discussion:  
  slides: 
  code: 
  data:  
-
  layout: lecture
  selected: y
  date: 2024-05-14
  img: bias
  uid: l10
  title: "Seminar: Bias in NLP models"
  instructor: "Vera Neplenbroek"
  note: 
  abstract: "In this session we will discuss research on bias in NLP models, diagnosing it and de-biasing."
  background:
  discussion:  
  slides: 
  further: 
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2024-05-17
  img: X-Cultural
  uid: l11
  title: "Seminar: Cross-cultural NLP"
  instructor: "Alina Leidinger"
  note: 
  abstract: "In this session we will discuss recent research on LLMs and their application beyond the English-speaking world."
  background:
  discussion: 
  slides: 
  further: 
  code: 
  data:    
-  
  layout: lecture
  selected: y
  date: 2024-05-21
  img: Clip
  uid: l12
  title: "Seminar: Language and Vision"
  instructor: "Alberto Testoni"
  note: 
  abstract: "In this session, we will discuss research on joint modelling of language and vision."
  background:
  discussion:
  slides: 
  code: 
  data: 
-  
  layout: lecture
  selected: y
  date: 2024-05-24
  img: Poster
  uid: l13
  title: "Project presentations"
  instructor: "Ekaterina Shutova, Vera Neplenbroek, Sara Rajaee and Alina Leidinger"
  note: 
  abstract: "In this session, you will present the results of your research projects."
  background:
  discussion:
  slides: 
  code: 
  data: 
