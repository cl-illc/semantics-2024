-
  layout: lecture
  selected: y
  date: 2024-04-02
  img: introduction-icon_1-267x300
  uid: intro
  title: "Introduction: Learning word and sentence representations"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this introductory lecture I will give an overview of the course and we will discuss learning word and sentence representations from text."
  background:
  discussion:
  slides: resources/slides/Sem2024-lecture1.pdf
  further: 
   -  "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. [A large annotated corpus for learning natural language inference](https://arxiv.org/pdf/1508.05326.pdf). arXiv preprint arXiv:1508.05326, 2015."
   - "Alexis Conneau and Douwe Kiela. [Senteval: An evaluation toolkit for universal sentence representations](https://www.aclweb.org/anthology/L18-1269.pdf). In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), 2018."
   - "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. [Supervised learning of universal sentence representations from natural language inference data](https://arxiv.org/pdf/1705.02364.pdf). arXiv preprint arXiv:1705.02364, 2017."
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2024-04-05
  img: bert
  uid: l2
  title: "Attention and transformers"
  instructor: "Phillip Lippe and Ekaterina Shutova"
  note: 
  abstract: "In this session we will introduce attention and transformer architectures"
  background:
  discussion:
  slides: 
  further:
    - "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. [Attention Is All You Need](https://arxiv.org/abs/1706.03762). In Proceedings of NIPS 2017."
    - "A very helpful [blog post](https://jalammar.github.io/illustrated-transformer/) explaining the transformer architecture."
    - "Visualization of attention heads for BERT: [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)"
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2024-04-09
  img: BERT
  uid: l3
  title: "Seminar: The BERT model"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss the BERT model."
  background:
  discussion: 
    - "In this session we will discuss the following papers:" 
    - "Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. 2019. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf). In Proceedings of NAACL 2019."
    - "Ian Tenney, Dipanjan Das, Ellie Pavlick. 2019.  [BERT Rediscovers the Classical NLP Pipeline](https://www.aclweb.org/anthology/P19-1452/). In Proceedings of ACL 2019."
  slides:
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2024-04-12
  img: GNN
  uid: l4
  title: "Seminar: Model pruning and modularity"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss recent techniques for structured and unstructured pruning and finding task-specific subnetworks in Transformer models."
  background:
  discussion: 
    - "In this session we will discuss the following papers:"
    - "Paul Michel, Omer Levy, Graham Neubig. [Are Sixteen Heads Really Better than One?](https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf) In Proceedings of NeuroIPS 2019."
    - "Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin. [The Lottery Ticket Hypothesis for Pre-trained BERT Networks.](https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf) In Proceedings of NeuroIPS 2020."
  slides: 
  further:
    - "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. (2020). [Gradient surgery for multi-task learning](https://arxiv.org/pdf/2001.06782.pdf). arXivpreprint arXiv:2001.06782."  
-
  layout: lecture
  selected: y
  date: 2024-04-16
  img: Multilingual
  uid: l5
  title: "Multilingual models"
  instructor: "Rochelle Choenni and Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss learning multilingual word and sentence representations."
  background:
  further:
    - "The paper that introduced byte pair encoding: Rico Sennrich, Barry Haddow, Alexandra Birch. 2016. [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf). In Proceedings of ACL 2016."
    - "Zirui Wang, Zachary C. Lipton, Yulia Tsvetkov (2020). [On Negative Interference in Multilingual Language Models](https://www.aclweb.org/anthology/2020.emnlp-main.359.pdf). In the proceedings of EMNLP 2020."
    - "Mikel Artetxe and Holger Schwenk. 2018. [Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond.](https://arxiv.org/pdf/1812.10464.pdf) Transactions of the Association for Computational Linguistics."
  discussion: 
  slides: 
  further:
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2023-04-25
  img: Meta-learning
  uid: l9
  title: "Meta-learning"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session I will introduce the meta-learning framerwork and we will discuss the application of meta-learning to NLP."
  background:
  slides: resources/slides/Sem2023-meta-learning-lecture.pdf
  further:  
    - "The paper by Finn et al (2017) introducing [model-agnostic meta-learning](https://arxiv.org/abs/1703.03400)."
    - "Jake Snell, Kevin Swersky, Richard S. Zemel. 2017 [Prototypical Networks for Few-shot Learning](https://arxiv.org/abs/1703.05175)."
    - "Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, Hugo Larochelle (2019).  [Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples](https://arxiv.org/abs/1903.03096)."
    - "Nithin Holla, Pushkar Mishra, Helen Yannakoudakis and Ekaterina Shutova. 2020. [Learning to Learn to Disambiguate: Meta-Learning for Few-Shot Word Sense Disambiguation.](https://arxiv.org/pdf/2004.14355.pdf) In Findings of EMNLP 2020."
    - "Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum. 2020. [Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks](https://arxiv.org/abs/2009.08445)."
    - "Nithin Holla, Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova. 2020. [Meta-Learning with Sparse Experience Replay for Lifelong Language Learning.](https://arxiv.org/pdf/2009.04891.pdf) In arXiv e-prints: 2009.04891." 
    - "Zirui Wang, Zachary C. Lipton, Yulia Tsvetkov (2020). [On Negative Interference in Multilingual Language Models](https://www.aclweb.org/anthology/2020.emnlp-main.359.pdf). In the proceedings of EMNLP 2020."
    - "Niels van der Heijden, Helen Yannakoudakis, Pushkar Mishra and Ekaterina Shutova. 2021.  [Multilingual and cross-lingual document classification: A meta-learning approach](https://arxiv.org/pdf/2101.11302.pdf). In Proceedings of EACL 2021. Online."
    - "Anna Langedijk, Verna Dankers, Phillip Lippe, Sander Bos, Bryan Cardenas Guevara, Helen Yannakoudakis and Ekaterina Shutova. 2021. [Meta-learning for fast cross-lingual adaptation in dependency parsing](https://arxiv.org/pdf/2104.04736.pdf). In arXiv e-prints: 2104.04736."
    - "Nooralahzadeh, F., Bekoulis, G., Bjerva, J., & Augenstein, I. (2020). [Zero-Shot Cross-Lingual Transfer with Meta Learning](https://arxiv.org/pdf/2003.02739.pdf). arXiv preprint arXiv:2003.02739."
  discussion:  
    - "In this session we will discuss the following papers:"
    - "Nooralahzadeh, F., Bekoulis, G., Bjerva, J., & Augenstein, I. (2020). [Zero-Shot Cross-Lingual Transfer with Meta Learning](https://arxiv.org/pdf/2003.02739.pdf). arXiv preprint arXiv:2003.02739."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2023-05-09
  img: MTL-NLP
  uid: l7
  title: "Seminar: Multitask learning"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss multitask learning, review several architectures proposed for it and see examples of language processing tasks that can benefit each other through information sharing."
  background:
  further:  
    - "Augenstein, I., Ruder, S., & Søgaard, A. (2018). Multi-Task Learning of Pairwise Sequence Classification Tasks over Disparate Label Spaces. In NAACL-HLT."
    - "Barrett, M., Bingel, J., Hollenstein, N., Rei, M., and Søgaard, A. (2018) Sequence classification with human attention. In Proceedings of the 22nd Conference on Computational Natural Language Learning (pages 302–312)"
    - "Bingel, J., and Søgaard, A. (2017). Identifying beneficial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers (pages 164-169)"
    - "Chen, Z., Badrinarayanan, V., Lee, C. Y., & Rabinovich, A. (2018, July). Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning (pp. 794-803). PMLR."
    - "Guo, M., Haque, A., Huang, D., Yeung, S., and Fei-Fei, L. (2018) Dynamic task prioritization for multitask learning. In European Conference on Computer Vision (pages 282–299)"
    - "Hu, R., & Singh, A. (2021). Transformer is all you need: Multimodal multitask learning with a unified transformer. arXiv preprint arXiv:2102.10772."
    - "Khandelwal, A., & Britto, B. K. (2020). Multitask Learning of Negation and Speculation using Transformers. In Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis (pp. 79-87)."
    - "Liu, S., Johns, E., & Davison, A. J. (2019). End-to-end multi-task learning with attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1871-1880)."
    - "Liu, X., He, P., Chen, W., & Gao, J. (2019). Multi-Task Deep Neural Networks for Natural Language Understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 4487-4496)."
    - "Mathias, S., Murthy, R., Kanojia, D., Mishra, A., & Bhattacharyya, P. (2020). Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing (pp. 858-872)."
    - "McCann, B., Keskar, N. S., Xiong, C., & Socher, R. (2018). The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730."
    - "Ruder, S., Bingel, J., Augenstein, I., & Søgaard, A. (2019, July). Latent multi-task architecture learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 4822-4829)."
    - "Sanh, V., Wolf, T., and Ruder, s. (2018) A hierarchical multi-task approach for learning embeddings from semantic tasks. In Thirty-Second AAAI Conference on Artificial Intelligence."
    - "Schröder, F., & Biemann, C. (2020). Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 2971-2985)."
    - "Sun, T., Shao, Y., Li, X., Liu, P., Yan, H., Qiu, X., & Huang, X. (2020). Learning sparse sharing architectures for multiple tasks. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 05, pp. 8936-8943)."
    - "Zhou, J., Zhang, Z., Zhao, H., & Zhang, S. (2020). LIMIT-BERT: Linguistics Informed Multi-Task BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings (pp. 4450-4461)."
  discussion:   
    - "In this session we will discuss the following papers:"
    - "Jonas Pfeiffer, Aishwarya Kamath, Andreas Ruckle, Kyunghyun Cho, Iryna Gurevych (2021). [AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://aclanthology.org/2021.eacl-main.39.pdf). In Proceedings of EACL 2021."
    - "Trapit Bansal, Rishikesh Jha, Andrew McCallum, 2020. [Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks](https://arxiv.org/pdf/1911.03863.pdf). Arxiv."
  slides: 
  code: 
  data:      
-
  layout: lecture
  selected: y
  date: 2023-05-12
  img: T0
  uid: l10
  title: "Seminar: Model prompting"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss recent research on model prompting, prompt-tuning and in-context learning."
  background:
  discussion: 
    - "In this session we will discuss the following papers:"
    - "Sanh et al., 2022. [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207.pdf). In Proceedings of ICLR 2022."
    - "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer. 2022. [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/pdf/2202.12837.pdf) In Proceedings of EMNLP 2022."
  slides: 
  further: 
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2023-05-23
  img: Discourse
  uid: l12
  title: "Seminar: Bias in NLP models"
  instructor: "Alina Leidinger and Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss research on bias in NLP models, diagnosing it and de-biasing."
  background:
  discussion:  
    - "In this session we will discuss the following papers:"
    - "Schick, Timo, Sahana Udupa, and Hinrich Schütze. [Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp.](https://aclanthology.org/2021.tacl-1.84.pdf) Transactions of the Association for Computational Linguistics 9 (2021): 1408-1424."
    - "Lauscher, Anne, Tobias Lüken, and Goran Glavaš. [Sustainable modular debiasing of language models.](https://arxiv.org/pdf/2109.03646.pdf) arXiv preprint arXiv:2109.03646 (2021)."
  slides: 
  further: 
  code: 
  data:  
-  
  layout: lecture
  selected: y
  date: 2023-05-26
  img: CoT
  uid: l13
  title: "Seminar: Chain-of-Thought prompting and In-Context Learning"
  instructor: "Konstantinos Papakostas"
  note: 
  abstract: "In this session, we will discuss research on Chain-of-Thought prompting and In-Context Learning."
  background:
  discussion:
    - "In this session we will discuss the following papers:"
    - "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf) In Proceedings of NeurIPS 2022."
    - "Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, Felix Hill. [Can language models learn from explanations in context?](https://aclanthology.org/2022.findings-emnlp.38.pdf) In Proceedings of Findings of EMNLP 2022."
  slides: 
  code: 
  data: 
-  
  layout: lecture
  selected: y
  date: 2023-06-02
  img: Poster
  uid: l14
  title: "Project presentations"
  instructor: "Ekaterina Shutova, Rochelle Choenni, Konstantinos Papakostas and Alina Leidinger"
  note: 
  abstract: "In this session, you will present the results of your research projects."
  background:
  discussion:
  slides: 
  code: 
  data: 
